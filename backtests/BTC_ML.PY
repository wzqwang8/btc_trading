"""
BTC ML Classifier — Predict +1% Price Move in 8 Hours
=======================================================
Features: MACD, RSI, std dev, OBV
Target:   1 if close[+8h] >= close * 1.01 else 0

Models compared via TimeSeriesSplit cross-validation:
  - Logistic Regression
  - Random Forest
  - Gradient Boosting
  - XGBoost

Best model selected on F1 score, then analysed with SHAP.

Data source: btc_gbp_hourly.csv (generated by cb_historical.py)
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import TimeSeriesSplit
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from imblearn.over_sampling import SMOTE
from xgboost import XGBClassifier
import shap

# ── Config ────────────────────────────────────────────────────────────────────
DATA_FILE      = os.path.join(os.path.dirname(os.path.abspath(__file__)), "btc_gbp_hourly.csv")
TARGET_HORIZON = 8      # bars ahead
TARGET_RETURN  = 0.01   # 1% move
N_SPLITS       = 5

# ── Load & feature engineering ────────────────────────────────────────────────
btc = pd.read_csv(DATA_FILE)
btc["time"] = pd.to_datetime(btc["time"])
btc.set_index("time", inplace=True)
btc.sort_index(inplace=True)

btc["ema12"] = btc["close"].ewm(span=12, adjust=False).mean()
btc["ema26"] = btc["close"].ewm(span=26, adjust=False).mean()
btc["macd"]  = btc["ema12"] - btc["ema26"]
btc["std"]   = btc["close"].rolling(14).std()

delta      = btc["close"].diff()
gain       = delta.where(delta > 0, 0).rolling(14).mean()
loss_r     = (-delta.where(delta < 0, 0)).rolling(14).mean()
btc["RSI"] = 100 - (100 / (1 + gain / loss_r))

btc["obv"] = (np.sign(btc["close"].diff()) * btc["volume"]).fillna(0).cumsum()

# Target: will price be ≥ +1% higher in TARGET_HORIZON bars?
btc["target"] = (btc["close"].shift(-TARGET_HORIZON) >= btc["close"] * (1 + TARGET_RETURN)).astype(int)

btc.dropna(inplace=True)
btc.drop(columns=["high", "low", "open", "ema26"], errors="ignore", inplace=True)

x = btc.drop(columns=["target"])
y = btc["target"]

print(f"Dataset: {len(btc):,} rows | Target balance: {y.mean():.1%} positive")

# ── Correlation heatmap ───────────────────────────────────────────────────────
plt.figure(figsize=(10, 8))
sns.heatmap(x.corr(), annot=True, fmt=".2f", cmap="coolwarm", square=True)
plt.title("Feature Correlation Matrix")
plt.tight_layout()
plt.show()

# ── Cross-validation ──────────────────────────────────────────────────────────
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Random Forest":       RandomForestClassifier(n_estimators=100, random_state=42),
    "Gradient Boosting":   GradientBoostingClassifier(random_state=42),
    "XGBoost":             XGBClassifier(eval_metric="logloss", random_state=42),
}

tscv         = TimeSeriesSplit(n_splits=N_SPLITS)
model_scores = {}

for name, model in models.items():
    print(f"\n{'='*40}\n{name}")
    acc_scores, prec_scores, rec_scores, f1_scores = [], [], [], []

    for train_idx, test_idx in tscv.split(x):
        x_train, x_test = x.iloc[train_idx], x.iloc[test_idx]
        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

        smote = SMOTE(random_state=42)
        x_res, y_res = smote.fit_resample(x_train, y_train)

        model.fit(x_res, y_res)
        y_pred = model.predict(x_test)

        acc_scores.append(accuracy_score(y_test, y_pred))
        prec_scores.append(precision_score(y_test, y_pred, zero_division=0))
        rec_scores.append(recall_score(y_test, y_pred, zero_division=0))
        f1_scores.append(f1_score(y_test, y_pred, zero_division=0))

    avg_f1 = np.mean(f1_scores)
    model_scores[name] = avg_f1
    print(f"  Accuracy:  {np.mean(acc_scores):.3f}")
    print(f"  Precision: {np.mean(prec_scores):.3f}")
    print(f"  Recall:    {np.mean(rec_scores):.3f}")
    print(f"  F1 Score:  {avg_f1:.3f}")

# ── Best model + SHAP ─────────────────────────────────────────────────────────
best_name  = max(model_scores, key=model_scores.get)
best_model = models[best_name]
print(f"\nBest model: {best_name} (F1 = {model_scores[best_name]:.3f})")

best_model.fit(x, y)
explainer   = shap.Explainer(best_model, x)
shap_values = explainer(x)

shap.summary_plot(shap_values, x, title=f"SHAP Feature Importance — {best_name}")
